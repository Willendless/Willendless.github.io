---
layout: "post"
title: 存储系统（七）可扩展存储系统和GFS
author: LJR
category: 存储系统
---

> Halt and catch file.

## 1. Aspects of decentralized services

### 1.1. concurrent accesses

+ how does one ensure consistent views of data
  + step 1: define consistency
    + SC: every read sees the most recent write?
  + step 2: enforce concurrency control
    + distributed synchronization primitives: we need to consider failure, re-ordered messages, etc

#### 1.1.1. concurrency control

+ option 1: central point updates
  + easily serialized
+ option 2: locks
  + what if server dies having granted a lock?
  + solution: **lease**, granted locks for a bounded time
+ option 3: cross fingers
+ option 4: **optimistic concurrency control**
  + assume no conflicts and verify before committing changes
  + problem: livelock
    + sequences that fail the verify step get rolled back
    + solution: fall back to pessimistic concurrency control

### 1.2. coordination and scaling

+ **scalability**: coordination of the servers should not be too interdependent
+ **reliability**: when there is inter-dependence(replication, parity, metadata), the servers must agree on the related state

#### 1.2.1. distributed consensus

+ two concerns
  + concurrency control
  + failures of servers
+ **distributed consensus for replicas**
  + option 1: primary and secondary servers, 主从备份
  + option 2: voting based
    + updates and reads go to quorum of servers
      + read quorum and write quorum should overlap
      + for any read, majority wins
  + option 3: 2-phase commit (distributed transaction)
    + send intention to all servers
    + send commit to all servers after all respond
    + read may be able to go to any up-to-date server

## 2. case study: GFS

**Application Driven Design**: simple and effective

### 2.1. goal

+ **component failures are the norm** -> constant monitoring
+ **append operation is the main focus of performance optimization** -> no client caching + atomic append
+ **co-design the application and file system API**
+ **high sustained bandwidth is more important than low latency**

### 2.2. architecture

![](/assets/images/ss/7-1.png)

+ client side: three kind
  + **in kernel**: nfs
  + **general purpose user level**: fuse
  + **linked library**: gfs
+ **components**
  + **single master**:
    + **maintain all system metadata**
      + namespace: **prefix table of path names**
      + access control information
      + mapping from files to chunks
      + current chunk locations
    + **system management**
      + lease
      + garbage collection of orphaned chunks
      + chunk migration of between chunkservers
  + **chunk servers**: inexpensive commodity components
    + store fixed-size chunk with linux file
    + read/write chunk data specified by a chunk handle and byte range
+ **naming mechanism**
  + **file to chunk handle**
  + **chunk handle to chunk location**
+ **caching mechanism**
  + client side
    + no file cache
    + caching file locations
  + server side
    + no file cache: only file system cache
    + caching all metadata in memory
  + why
    + most applications stream through data
    + working sets too large to fit into the cache

#### 2.2.1. chunk size

choose large chunk size: 64 MB

+ workload: large sequential read and write
+ reduce clients' need to interact with the master
+ more likely for clients to accumulate operations
+ reduce the size of the metadata stored on the master, so that can fit into memory

#### 2.2.2. metadata

+ metadata are all in-memory: **1:1,000,000 metadata to data size ratio**
  + **file and chunk namespace**: persistent
  + **mapping from file to chunk**: persistent
  + **locations of each chunk's replicas**: volatile, get by heartbeat message
+ **operation log**: record of metadata changes
  + replicate it on multiple machines
  + checkpoints: compact b-tree, can be mapped into memory for lookup

### 2.3. consistency model

For metadata

+ file namespace mutations are atomic
  + only performed by master
+ master's operation log defines a global total operations order

For file data

+ ask applications to only append data instead of overwriting
+ append guarantee
  + **defined**: clients can see a mutation writes in its entirety
  + **atomically at-least-once**: therefore GFS may insert padding or record duplicates

**Don't hide inconsistency from application.**

### 2.4. system interactions: dual ordering

**aim**: minimize the master's involvement in all operations

![](/assets/images/ss/7-2.png)

+ Use leases to maintain a consistent mutation order across replicas.
+ **primary**: chunkserver that be granted the lease by master
  + primary picks a serial order for all mutations
+ **global mutation order is defined by**
    1. the lease grant order chosen by the master
    2. within a lease by the serial numbers assigned by the primary

#### 2.4.1. control flow

1. client asks the master which server holds the current lease and the locations of the other replicas
2. master replies the location of primary and the other secondary replicas
3. client pushed the data to all the replicas using a pipeline fashion
4. once all the replicas ack the data, client sends a write request to the primary which identifies all the replicas. **the primary assigns serial numbers to all the mutations it receives, possibly from multiple clients**, then applies the mutation to it own local state in serial number order
5. primary forwards the write request to all other replicas, then they applies with the same serial number order
6. secondaries reply to the primary indicating that they have completed the operation
7. primary reply to the client. in case of errors, thw write may have succeeded at the primary and an arbitrary subset of replicas. then the client request is considered to be failed **and the modified region is left in an inconsistent state**. client code handles such errors by retrying the failed mutation (3. to 7.) before falling back to a retry from the beginning of the write

#### 2.4.2. data flow

+ from client -> replica A -> primary replica -> replica B
+ chained copying: use pipeline by distance to utilize full-duplex network bandwidth over TCP connections
+ design for poor cluster network bisection BW

#### 2.4.3. atomic record append

+ concurrent writes **to the same region** are not serializable, region may contain data fragments from multiple clients
+ record append: **client only specifies the data**
  + at least once atomicity
  + implementation
    + client push the data to each replica then send request to the primary
    + primary checks to see if the append will exceed the maximum size (64 MB). if so, it will pads the chunk to the maximum size, tells the secondaries to do the same, and tell the client indicating the operation should be retried on the next chunk (record append is restricted to 1/4 of chunk size to reduce fragmentation)
    + if the record fits within the maximum size, the **primary appends the data and tells the secondaries to write the data at the same offset, and finally replies success to the client**
  + if record append fails at any replica, the client retries. Therefore, replicas of the same chunk may contain different data possible including duplicates of the same record in whole or in part.

#### 2.4.4. snapshot

+ **snapshot**: makes a copy of a file or directory tree, while minimizing any interruptions of ongoing mutations
+ implementation
  + **cope-on-write**: when master receives a snapshot request, it first revokes any outstanding leases on the chunks in the files it is about to snapshot
  + then, the master logs the operation to disk
  + then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree, the newly created snapshot files point to the same chunks as the source files
  + later when client want to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder and the master will find the ref count for chunk C is greater than one. it defers replying to the client request and instead picks a new chunk handle C'. It then asks each chunkserver that has a current replica of C to create a new chunk called C' by copying the data of C locally. Then the master can grant one of the replicas a lease on the new chunk C' and replies to the client, which can write the chunk normally, not knowing that it has just been created from an existing chunk.

### 2.5. background maintenance

+ master monitor chunk locations and chunkserver status with **heartbeat** message
+ background analysis of each chunk
  + if any replicas are missing, re-replicate
  + gradual rebalancing load and disk usage
+ delete is just unlink: i.e. only change metadata
  + chunks are garbage collected later
  + until collected, it is still available (time travel)

### 2.6. a little bit of Colossus

+ GFS
  + large file bias induced complexity on apps
  + low latency apps poorly supported
+ no longer a single metadata server
  + shards metadata over many servers
  + use BigTable to store metadata
+ no longer simple replication
  + use file-based (instead of block based) reed-soloman (RAID 6 like) encoding
  + computes encoding at client
    + client sends "stripe units" rather than chained copying
