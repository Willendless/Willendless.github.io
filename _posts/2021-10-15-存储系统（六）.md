---
title: 存储系统（六）分布式文件系统
author: LJR
category: 存储系统
tags:
    - storage systems
---

> **End-to-end argument**: the highest level in a system is ultimately the only locale within a layered system where certain functionality can truly be implemented. 

Nature of Distributed System: **failure**

## 1. why distributed file system

+ **data sharing**
+ provisioning
  + easy to redistribute storage between users
+ management
  + backups easier when centralized
  + overall reliability higher
  + disaster recovery
+ key design decision
  + **how to partition FS functionality?**

## 2. single server

+ client-side caches
  + cache coherence
  + consistency model
    + **Sprite**: all reads see most recent write
    + **NFSv3**: other clients' writes visible in 30sec
    + **AFSv2**: reads see version closed most recently before open
    + **Original HTTP**: all reads see last read...i.e. no consistency

### 2.1. Approach 1: Server does everything

+ client requests simply forward to server, no caching
+ pros: simplicity
  + server looks same as kernel-based FS
+ cons: performance
  + performance of server can be bottleneck
  + performance of network can be bottleneck
  + memory state must be maintained for each client app session

### 2.2. Approach 2: Sprite (Caching + server control)

+ all reads see most recent write
+ implementation
  + clients tell server when they open a file for read or write
  + server tells client whether caching of the file is allowed
    + **multiple readers or a single writer**
  + disable all client caching of the file whenever concurrent write sharing might occur
  + clients tells server whenever file is closed
+ **server acts as a cache coherence controller**

### 2.3. Approach 3: NFS v2/v3 (stateless caching)

+ The goal of NFS is simple and fast **server crash recovery**.
+ **other clients'** writes visible within 30 seconds
  + clients cache blocks and remember last verification timestamp
  + server promises nothing and remember nothing
  + clients check block freshness whenever older than 30secs (via a GETATTR RPC)
+ **client write cache**
  + default: write-through cache
  + write back iff:
    + writes are flushed within a bounded time X
    + max time until writes are visible becomes 30+X is okay
+ functionality partition
  + client: provides session and caching
  + server: FS functionality

#### 2.3.1. server state

+ **stateful server**: have shared state between client and server
  + e.g. multiple sequential reads through a single file descriptor and server records the open file offset
+ **stateless server**: server doesn't track anything about what clients are doing
+ client
  + client records all the state through local file descriptors, using file-system-dependent code
  + file handler
    + **volume identifier**: the file system that requests refer to
    + **inode number**: which file within
    + **generation number**: for inode number reusing
+ server
  + no state

#### 2.3.2. handling server failure

+ how server handle server failure
  + **reboot is enough since there is no state at all in the server side**
+ how client handle **server failure** and **ack timeout**
  + **client just retries**
+ However, it should be noted that retry should be no side effect
  + **requests should be idempotent**
  + idempotent operations
    + LOOKUP
    + READ
    + WRITE
  + non-idempotent operation
    + MKDIR

**Stateless enables fast recovery, and idempotent operation enables retransmission.**

#### 2.3.3. caching

##### 2.3.3.1. client caching

+ **update visibility**: when should a client flush its updates to the server so that other clients can see
  + **flush on close**
+ **stale cache**: when should a client invalidate its cache line
  + **30 seconds after it is load into the cache line**
+ analysis
  + con 1: flush-on-close semantics lead to terrible performance for **short-lived files**
  + con 2: hard to reason if a file is in its latest version

##### 2.3.3.2. server-side write buffering

NFS servers can only return SUCCESS after flushing writes to stable storage. Otherwise, if the server fails and the dirty data in the buffer disappears, the writer client may not observe its write result even if it has received SUCCESS.

+ **write performance is terrible**
  + trick 1: use battery-backed memory
  + trick 2: use a file system design specifically designed to write to disk quickly

### 2.4. AFS v2(caching + callbacks)

+ idea: share is rare, all reads see most recent closed version before opened
  + use **whole-file caching** on the **local disk**: cache whole file into local disk and use client memory to hide disk latency
  + server promises to callback if the file changes
    + **interrupt model: so client doesn't need to check again**
  + client writes back entire file upon close
+ optimizations
  + use callback to invalidate stale client cache
  + use FID(similar to NFS file handle) to reduce server lookup cost
+ functionality partition
  + client: provides session and complete caching
  + server: does callbacks, directory management

### 2.5. differences between NFS and AFS

+ NFS
  + block level cache
  + only in memory
  + block level flush to server
  + 30s promise
  + client-self query updates: poll model
+ AFS
  + whole file level
  + in memory and disk
  + flush whole file to server when close
  + last closer wins
  + harder recovery
  + callback: interrupt model

## 3. multiple servers

+ why multiple servers
  + capacity
  + reliability
  + performance
+ from a high level: **partitioning of functionality across servers**
    1. do the same thing on each server, but for different data
    2. do something different on each server, but for the same data

### 3.1. approach 1: same function, different data

+ most common for distributed file system
+ naming and resource discovery
+ load balancing
  + manual data set placement
  + striping or pseudo-random distribution

#### 3.1.1. NFS v3

+ each NFS server serves its files independently
+ **naming**
  + clients need to mount each server's NFS filesystem, each server constructs its own namespace
  + Note: NFS servers are independent with each other (**servers don't know each other, i.e. stateless**)
+ **service discovery**
  + client traverses its own namespace and mount points
  + contacts server corresponding to given directory sub-tree
+ **load balancing**
  + very hard, system administrator can assign file sub-trees to servers and configure mount points on clients

#### 3.1.2. AFS

+ each AFS server serves its files independently
+ **naming**: **single namespace**
  + inter-volume linkages embedded in the global hierarchy
+ **service discovery**
  + traverse namespace and contact server that manages relevant volume at each point
  + determine managing server: client's **cache manager** lookup volume ID in **Volume Location Database** (maintained by **The Volume Location Server**)
+ **load balancing**
  + **system administrator** manually move volumes among servers

#### 3.1.3. HTTP

+ each HTTP server serves files independently
+ **naming**: single global namespace
+ **service discovery**
  + DNS + embedded file name in url
+ **load balancing**
  + possible only for subsets of namespace

### 3.2. approach 2: different function, same data

idea: change from file server to file manager, let file manager store metadata and get files directly from storage (SAN: storage area network)

![](/assets/images/ss/6-1.png)

+ more common in large-scale file system

#### 3.2.1. pNFS

![](/assets/images/ss/6-2.png)

+ client gets layout from server and perform direct I/O to storage
  + layout: map file onto storage devices and addresses
  + requirement: the server can recall the layout at any time
  + pNFS use is optional
