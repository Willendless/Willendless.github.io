---
layout: "post"
title: 存储系统（一）NAND-based SSD
author: LJR
category: 存储系统
tags:
  - storage systems
---

> 对寄存器和L1、L2 cache的访问在几纳秒数量级，L3 cache在几十纳秒，主存为100纳秒，SSD为100微秒，磁盘在10毫秒。

## 1. NAND-based Flash基础

SSD是固态硬盘技术的统称，可以有多种技术支持的固态硬盘，例如3D Xpoint，NAND-flash, NOR-flash等。这里涉及的是flash-based SSD。

### 1.1. NAND电路相关

数据存储原理: 数据以电子的形式通过一层浮置栅（floating gate）保存。由于被氧化膜保护因此电子能够保存很长时间。

![](/assets/images/ss/1-6.png)

floating gate的充放电是通过施加电压完成的。具体而言，NAND flash闪存利用隧道效应通过施加电压让电子能够穿过绝缘氧化层，达到放电/充电的目的。

![](/assets/images/ss/1-5.png)

+ **数据擦除**: 通过给衬底加正电压，让电子穿过氧化膜被释放。
+ **数据编程**: 通过给控制栅加正电压，从衬底获取电子。
+ **数据读取**: 给控制栅加读取电压。若浮栅中没有电子，则source和drain能够导通。若浮栅中有电子，则和控制栅的电压抵消，source和drain无法导通。
+ 注意: 编程和擦除的电压需要达到20V，而读取电压不需要这么大。

### 1.2. SSD存储组织

![](/assets/images/ss/1-7.png)

+ SSD存储属性
  + **package**: 整个SSD内部包含了多个package，上图就是一个package
  + **die/chip**: 每个SSD package包含多个**die(封装前的芯片)**。**不同的die能够并行操作**。die执行的每个操作能够涉及单个plane或多个plane（每种SSD不同）。
  + **plane**: 每个die包含多个**plane**，每个plane有一个page size的**寄存器**用于读取和写入操作。由于每个plane有自己的寄存器因此读写操作能够interleave执行。
  + **block**: 每个plane包含多个**block**，每个block包含多个**page**，一个page实际上是一行数据位。常见128kB或256kB。
  + **page**: 常见4kB，每个page包含（例如128字节）metadata（识别信息、循环冗余码）。
  + **8-bit serial connection**: 又称为channel
+ Flash基础操作
  + **读**（page）: **几十微秒**。支持随机访问任意页。**从存储介质读入寄存器**。但是数据通过串行总线上发送需要额外的时间（上图中100微秒）。
  + **擦除（erase）**（block）: **几毫秒**。擦除一个block会将其内每个bit位置1，因此擦除前需要保证内部有效数据被拷贝到其它位置（例如内存，或者另一个block）
  + **编程（program）**（page）: **几百微秒**。首先将数据发送到寄存器，然后写入对应页。
  + 注意: **块内的页通常要求按地址以从低到高的顺序写入**。
  + **page的状态**
    + 初始: Invalid
    + 擦除: Erased。将某个块内全置1，擦除后block内的各个page能写入。
    + 写入: Valid
    + 若想再次写入一个page，则需要再次擦除

注意，频繁的erase和program会导致flash很快**wear out**。因此设计存储系统时除了需要考虑**性能**，还需要考虑**可靠性**。

+ 可靠性问题1: **wear out**，erase和program会导致flash的氧化物隔离层逐渐被破坏，最终无法区分0和1
  + MLC-based为10,000 P/E，SLC-based 为100,000 P/E
+ 可靠性问题2: **disturbance**，read或者program页时，临近页中的某些位可能被翻转
  + **read disturbs**
  + **program disturbs**

### 1.3. Flash-based SSD

![](/assets/images/ss/1-1.png)

SSD内部包括了多个flash芯片，一些SRAM和控制逻辑。控制逻辑的作用在于将来自上层的对逻辑块的读写请求，转换为对物理block和page的read、erase和program命令。它被称为**FTL（flash translation layer）**。

+ **并行性**: 将连续数据分散存储在多个flash chip或是package上，提高并行性。
+ **写放大（write amplification）**: FTL发送给flash芯片写操作的字节数 **除以** 上层发送给FTL芯片写操作的字节数
+ **负载均衡，避免过早wear out**: FTL需要将写操作尽可能均匀分配到每个block，结果是SSD中每个block会在大致相同的时间不可用。这称为**wear leveling**。

**减小disturbance**: 顺序地对一个erased块内的page进行program。

### 1.4. Log structured FTL

**log structured**: 写操作被append到末尾，通过垃圾回收释放旧版本无效数据。

**开销1: 垃圾回收**

由于包含有无效数据的page必须擦除整个block后才能再次利用，因此需要垃圾回收。

找到一个包含有一个或多个垃圾页的block，从中读取剩下的仍然有效的页，将它们写入log，最后擦除该block。

`trim`向用户提供了告知FTL某个范围内的block已经不再需要的接口。FTL因此可以释放映射信息，之后在垃圾回收阶段释放对应空间。

**开销2: 逻辑块和物理块映射表**

+ **page-level mapping**
  + 每个逻辑块映射到一个页（FTL为每个物理页需要维护一个映射）。
  + **问题**: 若SSD容量为1TB，页大小为4kB，每个条目4字节，则映射表需要1GB。
+ **block based mapping**
  + 类似于虚拟地址到物理地址的映射。逻辑块号被划分成chuck number和offset两部分，offset给出在物理block内的page偏移。FTL只需要维护chuck number到物理block的映射。
  + **问题**: 如果需要写某个逻辑块，则需要将整个物理block读取出来，然后整个写入log，接着更新映射chunk的映射。对于小量的写操作，写放大会很高。
+ **hybrid mapping**
  + **log blocks**: FTL保留一部分erased block，对这部分采用per-page映射。对应的表称为**log table**
  + **data table**: 剩下的block使用block-based mapping
  + **地址翻译**: 当查找逻辑块时，首先查找log table然后查找data table
  + **switch操作**: 为了保证log block仅有较少的数量，FTL会周期性地检查log block，然后将其中可以被单个block指针索引的页聚合成单个block。
    + **switch merge**: 连续写入相同chunk号的逻辑块，原data table中的位置作为新的一个log block，被写入的log block作为一个新的data block。
    + **partial merge**: 只写入相同chunk号的部分逻辑块，从data table中读取剩余逻辑块，并merge。会导致额外的读写操作，增加写放大。
    + **full merge**: 写入非同一个chunk号的逻辑块，FTL需要额外寻找同一chunk号的其它逻辑块，然后共同构造data block。在做完之后，写入的log block才能被释放。
+ **page mapping plus caching**
  + 仅缓存FTL中积极的部分到内存中。
  + 如果FTL中的内存无法包含作业集，就会导致额外的读操作甚至牺牲内存中的映射（若是脏映射还会导致写操作）。

**开销3: wear leveling**

基本的log structured方法和垃圾回收已经较为有效。但是对于包含有长期有效数据的block，FTL需要周期性从中读取有效数据并将它们写入其它位置，这样该block才能够被再次写入。**注意**: 这个过程会增大写放大。

### 1.5. perfomance and cost

![](/assets/images/ss/1-2.png)

+ SSD的随机IO性能远好于HDD
+ SSD的顺序IO性能和HDD差别不大
+ SSD的随机读性能不如随机写性能好: 因为log structured下写操作均为append
+ SSD的随机和顺序IO性能仍存在差异，针对HDD的一些设计仍可以适用

**cost**

SSD单位容量开销较大，每GB大约是HDD的10多倍。如果对随机读写性能要求比较高，则SSD会比较适用。

通常可以混合使用SSD和HDD，使用SSD存储热数据以提供高性能，同时使用HDD存储较冷的数据以提供更大的空间。

## 2. 尝试深入理解

### 2.1. 分析1: 写放大

**设计1: 静态映射**

当执行rewrite操作时，必然需要

1. 读取整个块到dram
2. 擦除
3. 修改
3. 写回

因此对于小的、随机的写操作，写放大很严重。

**设计2: 以块为粒度映射**

当执行写操作的时候

1. 读取整个块
2. 修改
3. 写入

相比于静态映射，不需要立即进行擦除操作。写放大仍然非常大。由于每次均针对整个物理块操作，旧的物理块一定会无效，垃圾回收器可以直接回收。

**设计3: 以页为粒度映射**

当逻辑页的大小（从操作系统传递到FTL的单位数据粒度，多个逻辑块的大小）小于物理块的时候，物理块在擦除前需要先**clean**。即擦除前先将物理块内仍然有效的物理页写到其它位置。

当以页为粒度进行映射，执行写操作时，可以仅log writing新的页，并且更新映射。此时，垃圾收集器需要**explicitly clean**，而clean操作会导致写放大。

这种设计

+ SSD上需要更多内存存储映射
+ 需要设计clean的策略
  + **提高clean efficiency**: 无效页占被cleaning页的比例，减小写放大
  + **尽可能均衡，提高wear leveling**

**映射粒度对性能的影响**

+ block(256KB)
  + 平均IO延迟（TPC-C）为20ms
  + 显然大量的写放大
+ flash
  + 平均IO延迟（TPC-C）为0.2ms
  + 引入了cleaning的复杂性

### 2.2. 分析2: cleaning/垃圾回收

cleaning会严重影响随机写的性能，随着free erase块数量的减少，会频繁的进行cleaning操作。

SSD的存储设计需要尽可能避免foreground cleaning

+ foreground cleaning显然会带来更大的延迟
+ delayed cleaning通过聚集overwrites，导致更大的cleaning efficiency

减小cleaning的开销

+ **over-provisioning**（过度供应？）
  + 优势:
    + **垃圾回收**: 如果SSD被填充满了数据，则cleaning过程中SSD没有足够的空间存放待擦除块中的live data。因此减少foreground cleaning，能有更多执行background clean的可能。
      + 潜在地能让partially deleted块聚集更多的覆写。进而减少cleaning的开销。
    + **durability**: 使用保留块作为log和journals的位置，只有最后写入的数据被认为是正确的值，这些块直到log被写满都不需要擦除。
+ 提供**trim**接口
  + 传统上，os对存储数据的删除仅仅修改元数据，并将LBA放入空闲列表（os层面），而不通知SSD
  + solution: os发送**trim**命令以通知FTL
    + 减少cleaning开销，减少了无用的clean写操作
    + 减少了对overprovision的需要

### 2.3. 分析3: wear leveling

对于传统的Journaling file system，每次在执行写入操作前，文件系统会首先向固定的磁盘位置写日志，这个经常被写的位置被称为**hot spot**。其它的**hot spot**还包括block bitmap和inode bitmap。

由于传统的文件系统设计中不会考虑到SSD的wear out效应，因此SSD固件需要负责wear leveling的支持，也即组织hot spot，也类似于load balancing。

**durability**: 对于给定workload，能执行的擦除的次数。

**一个简单的leveling + cleaning算法**

+ 为每一个块设置一个clean上限，称为**migrate threshold**，同时维护一个clean次数的计数器。每次对该块clean时，将计数器减一，然后
+ 当$remaining(A) \ge Migrate-Threshold$，则进行clean操作，即复制有效数据到另一个块，擦除整个块，并能接受新的写入
+ 当$remaining(A) \lt Migrate-Threshold$，则复制有效数据到另一个块，然后**用冷数据块填充该数据块**

### 2.4. tradeoff小结

+ over-provisioning的意义
  + **用空间换clearning效率和durability**
+ todo

## 3. 实现细节

下面递增地介绍一种FTL的实现细节。

### 3.1. 细节1: 混合Log-Block映射机制

+ **log-reservation**: 使用overprovisioned blocks作为log block
+ **读操作**
  + 对每一个逻辑页的首次写操作静态映射到固定的data block页，例如对第一个页的写，映射到SSD上的`(0, 0, 0, 0, 0)`位置
  + 对同一逻辑页的第二次写操作
    + 第一步: 将静态映射的位置invalidate。
    + 第二步: 如果当前block不存在对应的log block则从overprovisioned block中分配一个。若overprovisioned block已用完，则返回错误。否则获取之前分配的log block。
    + 第三步: *尝试*顺序写入log block，若log block已满，则返回错误。
  + 读操作
    + 
+ **关键点**
  + **逻辑页映射**: 每个逻辑页（在SSD容量范围内，不包括overprovisioned区域）静态映射到物理页，即LBA->PBA的一一映射
  + **log block操作**
    + log block从over-provisioned blocks中分配
    + 每个data block最多分配有一个log block
    + log block需要顺序写入
+ 分析
  + data block采用one-to-one映射，因此只需要维护data block到log block的映射

### 3.2. 细节2: 垃圾回收

+ when to clean
  + on demand
+ how many blocks to clean
  + one
+ which blocks to clean
  + RR, LRU, greedy, LFS


### 3.3. 细节3: wear leveling

+ write amplification
+ maximum #writes before SSD defunct
+ uniform wear
+ memory usage
