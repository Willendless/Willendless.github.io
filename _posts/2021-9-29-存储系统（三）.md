---
title: 存储系统（三）file system layout
author: LJR
category: 存储系统
tags:
    - storage systems
---

> **File system storage layout**: map file system structures and data to LBNs

+ problem 1: mapping file data to storage locations.
+ problem 2: tracking and allocating free space.
  + solution 1: free list
  + solution 2: bitmap
  + solution 3: free extent list (separate list)
    + extent: a contiguous range of free blocks

## 1. storage layout for HDDs

![](/assets/images/ss/3-1.png)

+ HDDs
  + **problem**: how to thoroughly use disk's bandwidth?
  + **seek time depends on distance**: we need locality!
  + **positioning dominates small transfers**: we need larger transfer size!
+ two optimization technology
  + **locality**: minimize seeking between requests
    + **putting related things "close" to each other**
    + benefits: 2x range
  + **transfer size**: amortize positioning costs
    + **batch && prefetching**
    + benefits: 10x range

### 1.1. FFS

> Co-locate the file's data and metadata blocks so sequential reads will be quick. LFS, instead, will write data in the order it arrived.

![](/assets/images/ss/3-2.png)

+ locality: cylinder groups(allocation groups)
  + data block next to its inode block
  + file data next to its directory's inode and data
  + file within the same directory next to each other
  + **directory allocation**: find the group with a low number of allocated directories (**balance directories across groups**) and a high number of free inodes (to subsequently be able to allocate a bunch of files), and put directory inode and data into that group
  + **file allocation**
    + allocate file data blocks and inode in the same group
    + place all files that are in the same directory in the cylinder group of the directory
  + 注意: 现代磁盘驱动没有导出足够的让文件系统知道cylinder的使用情况的信息，因此文件系统仅仅是将连续的磁盘地址空间组织成block group。
+ large transfers
  + **larger FS block size**: more data per disk read/write
  + **allocate contiguous blocks when possible**
    + start allocation search from last block #
  + **prefetching**: fetch more data than requested

### 1.2. However

+ **small files remain a problem**, performance may degrade over 50% with aging
+ most files are small, 70% smaller than 8kB
  + levels of indirection (director, inode, data, ...)
+ Solution: **group small files and metadata for larger I/Os**
  + co-locate related information and access together
  + over aggressiveness costs little

### 1.3. LFS

> LFS optimize for write (change random write to sequential write and improve locality), and it serves read from a large cache.

+ motivation
  + **memories are growing**
  + **large gap between random IO performance and sequential IO performance**
  + **FFS performs poorly on write workloads**
    + create a new file: write a new inode, write to update the inode bitmap, write to the directory data block, write to the directory inode, write to the data block, write to the data block bitmap
  + **existing file system is not RAID aware**
    + small write problems

**Conclusion**: File system should focus on write performance and make use of sequential bandwidth.

**Write effectiveness**: To avoid rotation latency caused by interval between two sequential writes, we need to use **write buffering**, called segment.

+ idea: **amortization**, try to achieve large writes
  + buffer and remap new versions of data into large segments
  + segment-sized writes make efficient use disk head
+ an example of **shadow paging/copy-on-write**: not in-place update
  + completely write a segment to replace several older blocks
    + atomic writes help with consistency
  + **cleaning** reclaims space
    + how: **condense still-live blocks from several current segments**

#### 1.3.1. implementation details

##### 1.3.1.1. how much to buffer

![](/assets/images/ss/3-1.png)

It depends on how high the positioning overhead is in comparison to the transfer rate.

Suppose the positioning time is $T_{position}$, the bandwidth is $R_{peak}$ and the request size is D. Then the write time is

$$T_{write} = T_{position} + \frac{D}{R_{peak}}$$

Then the effective rate of writing (real bandwidth) is

$$R_{effectiveness} = \frac{D}{T_{write}} = \frac{D}{T_{position} + \frac{D}{R_{peak}}}$$

Suppose we want to achieve $F$ percent of peak bandwidth, then the buffer size we need can be calculated as

$$R_{effectiveness} = F * R_{peak}$$

and

$$D = \frac{F}{1-F}*R_{peak}*T_{position}$$

##### 1.3.1.2. how to find inodes

+ **old Unix file system**: inodes are organized in an array and in fixed location
+ **FFS**: splits up the inode table into chunks and places a group of inodes within each cylinder group
+ **LFS**: latest version of inode keeps moving
  + **inode map**: mapping from inode number to the disk address
    + location: chunks of the inode map are placed next to the newest version of the inode
  + **checkpoint region**: pointers to the latest pieces of the inode map
    + location: fixed area
    + update: periodically

![](/assets/images/ss/3-3.png)

+ take directory into consideration
  + **recursive update problem**: since directory contains only the inode number instead of the pointer to the inode, updating and changing the inode location doesn't need to change the directory itself
  + directory -> imap -> inode -> file

##### 1.3.1.3. garbage collection

LFS keeps only the latest live version of a file, therefore it needs to do garbage collection and the LFS cleaner works on a segment-by-segment basis.

Periodically, LFS reads in N segments, compact those live data within them to M segments (**N > M**), then write those M segments to the end of the log and clean N segments.

###### **Mechanism: How can LFS tell which blocks within a segment are live or dead?**

Given a data block D within an on-disk segment S, LFS records its inode number(**反向索引，之前是inode到block的映射**) and its offset at the head of the segment known as the **segment summary block**.

By comparing the latest version of the data location of the offset within a file (this can be calculated using imap，即正向索引), we can determine whether they are stale data.

```c
(inode_number, offset) = SegmentSummary[BLOCK_A];
inode = Read(imap[inode_number])
if (inode[offset] == A)
  // block D is alive
else
  // block D is garbage
```

**optimization**: using version number when creating or truncating file

###### **Policy: Which blocks to clean and when**

+ utilization
+ hot/cold
  + hot: wait for a longer time to clean them so than we can accumulate more overwrites
  + cold: clean those with low utilization

##### 1.3.1.4. crash recovery and the log

log: **checkpoint region points to a head and tail segment**, and each segment points to the next segment to be written.

+ **atomically update CR**
  + LFS keeps two CRs and writes to them alternately
  + when updating CR, also writing timestamps in the head and tail of CR to ensure whether consistent
+ checkpoint region is written to disk every 30 seconds or so
  + optimization: **roll forward from the last segment pointed by the checkpoint region and see if there are any valid updates**

## 2. storage layout for flash SSDs

+ mechanical performance drivers aren't present
+ modern FTLs do LFS-like remapping into SSD blocks

### 2.1. F2FS

+ Trick 1: **set segment size = SSD block size**
  + fine-grained mapping and cleaning only in F2FS
+ Trick 2: **separate hot/cold data**
  + put hot data in different segments than cold data
