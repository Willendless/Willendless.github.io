---
layout: "post"
title: 存储系统（十一）LSM树及应用
author: LJR
category: 存储系统
---

> LSM trees are designed to provide better write throughput than traditional B+ tree by removing the need to perform dispersed, update-in-place operations.

无意中发现BigTable是当年OSDI的best paper，亏我看的时候还感觉有点不如GFS那么通顺和结构化，太菜了。。一作是PDL的phd学长，太强了，Orz。GFS的二作也是PDL的phd学长/教授，哭了，我竟能有机会在这么厉害的学校上课，这学期还上的PDL实验室开的的存储系统课，有点不怎么真实的感觉！！！最近状态不是很好，没有好好学习，希望自己能加油！其实也不知道自己的未来是什么样的，有点迷茫和自卑。。。只能安慰自己，don't panic。

+ we need sequential access for better throughput and bandwidth
+ not just write-ahead-log for simple workload like accessing log in its entirety, or by a known offset, since **random read from log will be far more time consuming than writing to it**
  + requirement: more complex read workloads
    + key-based access
    + range search
  + previous approaches
    + search sorted file: save data to a file, sorted by key
    + hash: split the data into buckets using a hash function
    + b+ tree
    + external file and index: leave the data as a log/heap and create a separate hash or tree index
  + result: better read performance (log(n)) with more overhead of write
    + write needs first read then write
    + need to update the structure of the hash or B+ index, a.k.a **update file in place**
    + exception: index into a journal and keep it in memory but scalability limits.
+ append-only Btree (copy-on-write tree): overwriting the tree structure at the end of the file each time a write occurs, causing **bigger write amplification**
+ LSM tree: log structured merge-tree
  + little in memory storage for efficiency
  + keep much of the write performance
  + poorer read performance than B+ tree

## 1. LSM tree

是时候再次祭出这张图了。

![](/assets/images/ss/9-2.png)

+ SSTable: sorted string table
+ **write operation**: batches of writes (**data stream of k-v pairs**) are buffered in memtable, then sorted and go to immutable smaller index files
  + memtable
    + tree structure (red black tree, etc) to preserve key-ordering
    + replicated on disk as a write-ahead-log for recovery
  + periodically compact multiple files to garbage collection
+ **read operation**: check memtable, then files one by one, until the key is found
  + in general, read performance is worse than in-place
  + **optimization 1**: hold a page-index in memory
    + bigtable: choose 64kb as block size and store block-index at the end of each file, which works better than straight binary search
  + **optimization 2**: periodically merge file together
    + since file is sorted, merging is efficient
  + **optimization 3**: bloom filter, a memory efficient way of working out whether a file contains a key

### 1.1. compaction analysis

+ the on-disk data is organized into sorted runs of data
  + **run**: each run contains data sorted by the index key. a run an be one single file, or a collection of files with non-overlapping key ranges
+ **size-tiered compaction**
  + each file has fixed size
  + when the data size of one level reaches the limit, the whole level is merged and flushed to the next level
  + result: create a lot of files, all must be searched to read a result
  + pro: **suitable for write-intensive workloads**
  + con: greater read amplification and space amplification
+ **levelled compaction**: reduces the number of files that must be consulted for the worst case read
  + the size of each level is maintained at T times that of the previous level
  + in each level expect for level 0, keys are partitioned across the available files, when the size of level L reaches the upper limit, the compaction picks a file from level L and all overlapping files from the next level L+1 and merge them to produce a sequence of level L+1 files, the old files are discarded
    + find a key in a certain level only require consult one file
  + pro: when compaction, combines multiple runs of the level to one, reducing the read amplification and space amplification
  + pro: smaller sstables provide fine-grained task splitting and control
  + con: greater write amplification

### 1.2. operations

#### 1.2.1. **insertion**

1. write sequentially
2. sort data for quick lookups
3. sorting and garbage collection are coupled

![](/assets/images/ss/11-2.png)

+ per-SST index: list first & last key per SSTable
+ compaction
  + merge-sorts selected SSTs from level i and i+1
  + aims to ensure L1+ SSTs represent distinct key ranges

#### 1.2.2. **lookup: O(log)**

1. random reads
2. travel many levels for a large LSM-tree

数据所在层数越深，数据上一次更新越老。

![](/assets/images/ss/11-1.png)

+ per-SST index
  + cacheable bloom filter
  + bloom filter: if no, then the key is guaranteed to not include in the file. can skip ~99% unneeded lookups

## 2. applications: tablefs

+ **Small objects** stored into LSM tree
  + directory entires, inodes, small files
  + **small files can be accumulated to large SSTables**
+ **Large files** stored into object store

### 2.1. table schema

![](/assets/images/ss/11-4.png)

+ **key**: (parent inode number, hash(filename))
  + **files within the same directory are next to each other**
  + inodes with multiple hard links: (parent inode number, null)
+ **value**: filename, inode attributes, small file, large file pointer

### 2.2. architecture

![](/assets/images/ss/11-3.png)

+ layer above local file systems
  + FUSE
  + levelDB

### 2.3. evaluation

+ workload: 2 million random lookups (stat), or updates (chmod/utime)
+ tableFS 1.5-10x faster than other FSs
+ analysis
  + **LSM Tree reduces random disk writes**
  + **Reducing random disk reads**
    + co-locating directory entries with inodes (prefetching)
    + bloom filters avoid wasted lookups

## 3. reference

+ [Log Structured Merge Trees](http://www.benstopford.com/2015/02/14/log-structured-merge-trees/)
+ [wiki](https://en.wikipedia.org/wiki/Log-structured_merge-tree)
+ [leveldb doc](https://github.com/google/leveldb/blob/master/doc/impl.md)
